<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pedro F. Proença</title>
    <link>https://pedropro.github.io/</link>
    <description>Recent content on Pedro F. Proença</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Pedro F. Proença</copyright>
    <lastBuildDate>Wed, 10 Jul 2019 14:33:10 +0100</lastBuildDate>
    
	    <atom:link href="https://pedropro.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering</title>
      <link>https://pedropro.github.io/publication/urso/</link>
      <pubDate>Wed, 10 Jul 2019 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/urso/</guid>
      <description></description>
    </item>
    
    <item>
      <title>URSO</title>
      <link>https://pedropro.github.io/project/urso/</link>
      <pubDate>Fri, 31 May 2019 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/urso/</guid>
      <description>

&lt;p align=&#34;justify&#34;&gt;
&lt;i&gt;
URSO (Unreal Rendered Spacecraft On-Orbit) is a simulator built on Unreal Engine 4 to render photorealistic images of spacecrafts orbiting the earth, which can be used to learn and evaluate spacecraft pose estimation, tracking and detection algorithms.&lt;/i&gt;
&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/hU-_4zQUz3Q&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
This work is part of: &lt;/br&gt;
&lt;small&gt;
P. Proença and Y. Gao, &lt;i&gt;Deep Learning for Spacecraft Pose Estimation from Photorealistic Rendering&lt;/i&gt;, arXiv:1907.04298, 2019.  &lt;/small&gt;
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://arxiv.org/abs/1907.04298&#34; rel=&#34;noopener&#34;&gt; PDF &lt;/a&gt;
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://doi.org/10.5281/zenodo.3279632&#34; rel=&#34;noopener&#34;&gt; Dataset&lt;/a&gt;
&lt;a class=&#34;btn btn-outline-primary my-1 mr-1 btn-sm&#34; href=&#34;https://github.com/pedropro/UrsoNet&#34; rel=&#34;noopener&#34;&gt; Code&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;context&#34;&gt;Context&lt;/h2&gt;

&lt;p align=&#34;justify&#34;&gt;
Spacecraft pose estimation is an important task to on-orbit proximity maneuvers in rendezvous/docking and servicing operations, but also future space debris removal missions. While this has been mostly addressed by using classic computer vision techniques based on edge-models and hand-engineered features, these methods are not robust to the challenging lighting conditions of space and the earth background. This was recently acknowledged by ESA and Stanford SLAB who joined forces to open a &lt;a href=&#34;https://kelvins.esa.int/&#34;&gt;competition&lt;/a&gt; to solve this problem using deep learning.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;p&gt;The video below gives a glimpse of how deep learning models trained on URSO datasets perform on real space imagery.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/x8IbxmOz730&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For more information and results check our paper. This is ongoing work, we plan to release more data and a version of the simulator in a near future.
&lt;p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Odometry</title>
      <link>https://pedropro.github.io/project/vo/</link>
      <pubDate>Fri, 31 May 2019 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/vo/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt;
During my PhD, I developed several methods to extract and use geometric entities such as line segments, planes and cylinders for RGB-D odometry, which proved to yield robustness to textureless surfaces, motion blur and missing/noisy depth measurements. The video below summarizes well the localization results of using my system with feature points, lines and planes.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Y0T2_ghlng0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
This works remarkably well for a frame-to-frame method, but what happens when instead of planes, the scene is made of curved surfaces, e.g., tunnels and pipelines? Well, It turns out that the employed plane extraction method actually deteriorates the visual odometry performance. Therefore, I developed CAPE, a plane and cylinder extraction method that is curve-aware and 4-10 faster than state-of-the-art. For all scenes shown in the video below, CAPE improves the VO performance.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/FPFPVwm_yq0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
Now, typically one would reduce the drift by extending this work to full SLAM, however there is another way. We can use instead these geometric primitives to do model tracking if we have an accurate floor plan of a building. I have done some preliminary experiments, as shown below, of combining frame-to-frame visual odometry with model tracking by extruding a mesh model from a floor plan and render this in real time through OpenGL. The green lines correspond to the edges of the rendered model.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vMsV04emXHU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
Why should I do that: (i) zero-drift navigation without backend optimization, (ii) meaningful pose information, i.e., relative to the physical space.
&lt;/p&gt;

&lt;!-- Zero-drift by combining visual odometry with model tracking

A mesh model extruded from a floor plan is rendered through OpenGL for model alignment/tracking with planes and lines while frame-2-frame VO is used for smoothing. Green lines correspond to the mesh edges. Notice how pose errors are corrected.
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/vMsV04emXHU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</title>
      <link>https://pedropro.github.io/publication/cape/</link>
      <pubDate>Mon, 31 Dec 2018 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/cape/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TACO</title>
      <link>https://pedropro.github.io/project/taco/</link>
      <pubDate>Thu, 31 May 2018 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/taco/</guid>
      <description>

&lt;h2 id=&#34;coming-soon&#34;&gt;Coming Soon&lt;/h2&gt;

&lt;!-- Zero-drift by combining visual odometry with model tracking

A mesh model extruded from a floor plan is rendered through OpenGL for model alignment/tracking with planes and lines while frame-2-frame VO is used for smoothing. Green lines correspond to the mesh edges. Notice how pose errors are corrected.
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/vMsV04emXHU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic RGB-D Odometry based on Points, Lines and Planes Under Depth Uncertainty</title>
      <link>https://pedropro.github.io/publication/ras_18_paper/</link>
      <pubDate>Sat, 10 Mar 2018 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/ras_18_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation from RGB-D Camera Motion</title>
      <link>https://pedropro.github.io/publication/splode/</link>
      <pubDate>Thu, 14 Dec 2017 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/splode/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
