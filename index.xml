<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pedro F. Proença</title>
    <link>https://pedropro.github.io/</link>
    <description>Recent content on Pedro F. Proença</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 May 2019 18:37:52 +0100</lastBuildDate>
    
	    <atom:link href="https://pedropro.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>URSO</title>
      <link>https://pedropro.github.io/project/urso/</link>
      <pubDate>Fri, 31 May 2019 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/urso/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt;
URSO (Unreal Rendered Spacecraft On-Orbit) is a simulator built on Unreal Engine 4 to render photorealistic images of spacecrafts orbiting the earth, that
can be used to learn and evaluate spacecraft pose estimation, tracking and detection algorithms.
&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/hU-_4zQUz3Q&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Spacecraft pose estimation is an important task to on-orbit proximity maneuvers in rendezvous/docking and servicing operations, but also future space debris removal missions. While this has been mostly addressed by using classic computer vision techniques based on edge-models, these methods are not robust to the challenging lighting conditions of space and the earth background. Therefore, I have been training instead deep learning models on URSO datasets for these tasks and investigating how to better transfer these to real space imagery. Below, you can see some results of these experiments.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/x8IbxmOz730&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Odometry</title>
      <link>https://pedropro.github.io/project/vo/</link>
      <pubDate>Fri, 31 May 2019 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/vo/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt;
During my PhD, I developed several methods to extract and use geometric entities such as line segments, planes and cylinders for RGB-D odometry, which proved to yield robustness to textureless surfaces, motion blur and missing/noisy depth measurements. The video below summarizes well the localization results of using my system with feature points, lines and planes.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Y0T2_ghlng0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
This works remarkably well for a frame-to-frame method, but what happens when instead of planes, the scene is made of curved surfaces, e.g., tunnels and pipelines? Well, It turns out that the employed plane extraction method actually deteriorates the visual odometry performance. Therefore, I developed CAPE, a plane and cylinder extraction method that is curve-aware and 4-10 faster than state-of-the-art. For all scenes shown in the video below, CAPE improves the VO performance.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/FPFPVwm_yq0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
Now, typically one would reduce the drift by extending this work to full SLAM, however there is another way. We can use instead these geometric primitives to do model tracking if we have an accurate floor plan of a building. I have done some preliminary experiments, as shown below, of combining frame-to-frame visual odometry with model tracking by extruding a mesh model from a floor plan and render this in real time through OpenGL. The green lines correspond to the edges of the rendered model.
&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vMsV04emXHU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/p&gt;
&lt;p&gt;
Why should I do that: (i) zero-drift navigation without backend optimization, (ii) meaningful pose information, i.e., relative to the physical space.
&lt;/p&gt;

&lt;!-- Zero-drift by combining visual odometry with model tracking

A mesh model extruded from a floor plan is rendered through OpenGL for model alignment/tracking with planes and lines while frame-2-frame VO is used for smoothing. Green lines correspond to the mesh edges. Notice how pose errors are corrected.
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/vMsV04emXHU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry</title>
      <link>https://pedropro.github.io/publication/cape/</link>
      <pubDate>Mon, 31 Dec 2018 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/cape/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TACO</title>
      <link>https://pedropro.github.io/project/taco/</link>
      <pubDate>Thu, 31 May 2018 18:37:52 +0100</pubDate>
      
      <guid>https://pedropro.github.io/project/taco/</guid>
      <description>

&lt;h2 id=&#34;coming-soon&#34;&gt;Coming Soon&lt;/h2&gt;

&lt;!-- Zero-drift by combining visual odometry with model tracking

A mesh model extruded from a floor plan is rendered through OpenGL for model alignment/tracking with planes and lines while frame-2-frame VO is used for smoothing. Green lines correspond to the mesh edges. Notice how pose errors are corrected.
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/vMsV04emXHU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
 --&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic RGB-D Odometry based on Points, Lines and Planes Under Depth Uncertainty</title>
      <link>https://pedropro.github.io/publication/ras_18_paper/</link>
      <pubDate>Sat, 10 Mar 2018 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/ras_18_paper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation from RGB-D Camera Motion</title>
      <link>https://pedropro.github.io/publication/splode/</link>
      <pubDate>Thu, 14 Dec 2017 14:33:10 +0100</pubDate>
      
      <guid>https://pedropro.github.io/publication/splode/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
